
log_interval: 10
save_epoch: 1
num_worker: 2
log_time_cost: False
threshold: 0.55
cache_num: 0
crop_face:
  face_width: 80
  output_size: 224
  scale: 0.9

test:
  dataset:
    DFDC: './test_dfdc'
    CELEBDF: './all_celebdf'
  batch_size: 4

train:
  batch_size: 8
  epoch_num: 200
  use_SBI: True
  dataset:
    FF++: './ffplus'
    DFDC: './all_dfdc'

adm_det:
  min_dim: 224
  aspect_ratios: [[1], [1], [1], [1]]
  feature_maps: [7, 5, 3, 1]
  steps: [32, 45, 75, 224]
  min_sizes: [40, 80, 120, 224]
  max_sizes: [80, 120, 160, 224]
  clip: True
  variance: [0.1]
  name: "deepfake"

sliding_win:
  prior_bbox: [[40, 80], [80, 120], [120, 160], [224, 224]]

det_loss:
  num_classes: 2
  overlap_thresh: 0.9
  prior_for_matching: True
  bkg_label: 0
  neg_mining: True
  neg_pos: 2
  neg_overlap: 0.5
  encode_target: False
  use_gpu: True

model:
  name: "cross_vit_caddm"
  backbone: "resnet34"
  save_path: "./checkpoints"
  image-size: 448
  num-classes: 2
  depth: 4              # number of multi-scale encoding blocks
  sm-dim: 192            # high res dimension
  sm-patch-size: 7      # high res patch size (should be smaller than lg-patch-size)
  sm-enc-depth: 2        # high res depth
  sm-enc-dim-head: 64        
  sm-enc-heads: 8        # high res heads
  sm-enc-mlp-dim: 2048   # high res feedforward dimension
  lg-dim: 384            # low res dimension
  lg-patch-size: 56      # low res patch size
  lg-enc-depth: 3        # low res depth
  lg-enc-dim-head: 64    
  lg-enc-heads: 8        # low res heads
  lg-enc-mlp-dim: 2048   # low res feedforward dimensions
  cross-attn-depth: 2    # cross attention rounds
  cross-attn-dim-head: 64    
  cross-attn-heads: 8    # cross attention heads
  lg-channels: 24
  sm-channels: 1280
  dropout: 0.15
  emb-dropout: 0.15